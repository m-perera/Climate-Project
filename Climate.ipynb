{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "KryaSBjbLtVU",
      "metadata": {
        "id": "KryaSBjbLtVU"
      },
      "source": [
        "**Import** **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffcfe92a-4e79-420d-b93b-b71d8faa0ff1",
      "metadata": {
        "id": "ffcfe92a-4e79-420d-b93b-b71d8faa0ff1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import umap\n",
        "from sklearn.manifold import trustworthiness\n",
        "import hdbscan\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from scipy import stats\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.metrics import davies_bouldin_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QQkFOzpjL1wj",
      "metadata": {
        "id": "QQkFOzpjL1wj"
      },
      "source": [
        "**Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oTl1obJrI5Bi",
      "metadata": {
        "id": "oTl1obJrI5Bi"
      },
      "outputs": [],
      "source": [
        "# Get the current working directory\n",
        "current_wd = os.getcwd()\n",
        "\n",
        "# Define paths relative to the base directory\n",
        "climate_data_dir = os.path.join(current_wd, \"Project\")\n",
        "climate_data = os.path.join(climate_data_dir,\"Climate\",\"ClimateDataBasel.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee899dab-79b0-43ea-b88f-8f92e7b12ab1",
      "metadata": {
        "id": "ee899dab-79b0-43ea-b88f-8f92e7b12ab1"
      },
      "outputs": [],
      "source": [
        "#Store Features names in feature variable\n",
        "features = [\"Temperature (Min) oC\",\"Temperature (Max) oC.\",\"Temperature (Mean) oC.\",\"Relative Humidity (Min) %\",\"Relative Humidity (Max) %\",\"Relative Humidity (Mean) %\",\"Sea Level Pressure (Min) hPa\",\"Sea Level Pressure (Max) hPa\",\"Sea Level Pressure (Mean) hPa\",\"Precipitation Total mm\",\"Snowfall Amount cm\",\"Sunshine Duration min\",\"Wind Gust (Min) Km/h\",\"Wind Gust (Max) Km/h\",\"Wind Gust (Mean) Km/h\",\"Wind Speed (Min) Km/h\",\"Wind Speed (Max) Km/h\",\"Wind Speed (Mean) Km/h\"]\n",
        "#read csv and store in df variable\n",
        "df = pd.read_csv(climate_data, names= features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vqov1j_jKOsX",
      "metadata": {
        "id": "vqov1j_jKOsX"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca579e43-b49b-41c8-a85d-2027fcded955",
      "metadata": {
        "collapsed": true,
        "id": "ca579e43-b49b-41c8-a85d-2027fcded955"
      },
      "outputs": [],
      "source": [
        "# Check for missing:\n",
        "missing = df.isnull().sum() # There are none\n",
        "print(missing)\n",
        "\n",
        "\n",
        "# Check for duplicated:\n",
        "duplicated = df.duplicated().sum() #There are none\n",
        "print(duplicated)\n",
        "\n",
        "\n",
        "#Outlier Detection:\n",
        "isolation_forest = IsolationForest(contamination=\"auto\",random_state=23)\n",
        "\n",
        "isolation_forest.fit(df)\n",
        "\n",
        "predict_df = isolation_forest.predict(df)\n",
        "\n",
        "df[\"predict\"] = predict_df\n",
        "\n",
        "outliers = sum(predict_df == -1)\n",
        "\n",
        "print(outliers)\n",
        "\n",
        "#Remove Outliers:\n",
        "df = df[df[\"predict\"]==1] # remove outliers\n",
        "df = df.drop(\"predict\", axis=1) #remove the predict (1/-1) outlier column\n",
        "\n",
        "#Feature Selection/Extraction:\n",
        "variance_level = 5 # this was bassed on print(variance_per_feature)\n",
        "variance_per_feature = df.var()\n",
        "print(variance_per_feature)\n",
        "\n",
        "get_rid = variance_per_feature[variance_per_feature <= variance_level].index.tolist()\n",
        "print(get_rid)\n",
        "\n",
        "#Remove Features with lower variance then 5 from the DataFrame\n",
        "df = df.drop('Precipitation Total mm', axis=1)\n",
        "df = df.drop('Snowfall Amount cm', axis=1)\n",
        "df = df.drop('Wind Gust (Min) Km/h', axis=1)\n",
        "df = df.drop('Wind Speed (Min) Km/h', axis=1)\n",
        "df = df.drop('Wind Speed (Mean) Km/h', axis=1)\n",
        "\n",
        "\n",
        "#Feature Engineering:\n",
        "\n",
        "#Season (Summer/Winter)\n",
        "temp = 13 # lowest temp in summer (June)\n",
        "df[\"Season(Summer (1) or Winter(0))\"] = ((df[\"Temperature (Min) oC\"] >= temp).astype(int))\n",
        "\n",
        "#Temperature Range\n",
        "df[\"Temperature Range\"] = (df[\"Temperature (Max) oC.\"]-df[\"Temperature (Min) oC\"])\n",
        "\n",
        "\n",
        "#Scaling:\n",
        "\n",
        "#First determine the distribution to then determine the scaling option\n",
        "#Shapiro - Wilk Test\n",
        "\n",
        "normal_distribution_test = stats.shapiro(df)\n",
        "stat = normal_distribution_test.statistic\n",
        "p_value = normal_distribution_test.pvalue\n",
        "\n",
        "print(p_value)   # if p-value > 0.05, fail reject null data is likley normally dis, if p-value <= 0.05 reject null, data is no normally distributed\n",
        "print(stat)  # indicates how closely the sameple distribuation matches a normal distribution. In general, values closer to 1 suggest that the data is more likey to be normal distributed\n",
        "\n",
        "#output of the p_value and stat was 5.069605338919966e-115 and 0.5971794215682675, so reject null it is not normally distributed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EynJHUVvWzDo",
      "metadata": {
        "id": "EynJHUVvWzDo"
      },
      "outputs": [],
      "source": [
        "#Now apply RobustScaler\n",
        "updated_features = df.columns.values.tolist()\n",
        "\n",
        "rbscaler = RobustScaler()\n",
        "\n",
        "non_binary_features = df.columns[df.columns != \"Season(Summer (1) or Winter(0))\"]\n",
        "\n",
        "df[non_binary_features] = rbscaler.fit_transform(df[non_binary_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcF-lZu7YRkT",
      "metadata": {
        "collapsed": true,
        "id": "dcF-lZu7YRkT"
      },
      "outputs": [],
      "source": [
        "#Dimension Reduction (PCA)\n",
        "\n",
        "#Drop the binary column\n",
        "other_features = []\n",
        "binary_features = [\"Season(Summer (1) or Winter(0))\"]\n",
        "\n",
        "for feature in updated_features:\n",
        "  if feature not in binary_features:\n",
        "    other_features.append(feature)\n",
        "\n",
        "other_features_data = df[other_features]\n",
        "\n",
        "#PCA\n",
        "pca = PCA(n_components=0.95) # we want to have 95% of the variance explained\n",
        "pca_results = pca.fit_transform(other_features_data) # results\n",
        "\n",
        "components_used = pca.n_components_\n",
        "print(components_used) # used 7 components to explain 95% of the variance\n",
        "\n",
        "#Turn results to a dataframe\n",
        "pca_df = pd.DataFrame(pca_results, columns=[\"PCA1\",\"PCA2\",\"PCA3\",\"PCA4\",\"PCA5\",\"PCA6\",\"PCA7\"])\n",
        "\n",
        "#Add back in the binary (season)\n",
        "pca_df[\"Season\"] = df[\"Season(Summer (1) or Winter(0))\"].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HirrA7WPj6As",
      "metadata": {
        "collapsed": true,
        "id": "HirrA7WPj6As"
      },
      "outputs": [],
      "source": [
        "#Visualize\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(x=\"PCA1\",y=\"PCA2\", hue=\"Season\", data=pca_df, s=100, markers=\"o\")\n",
        "plt.title(label=\"PCA 1 and 2 Plot\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(x=\"PCA1\",y=\"PCA3\", hue=\"Season\", data=pca_df, s=100, markers=\"o\")\n",
        "plt.title(label=\"PCA 1 and 3 Plot\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 3\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(x=\"PCA2\",y=\"PCA3\", hue=\"Season\", data=pca_df, s=100, markers=\"o\")\n",
        "plt.title(label=\"PCA 2 and 3 Plot\")\n",
        "plt.xlabel(\"PCA Component 2\")\n",
        "plt.ylabel(\"PCA Component 3\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba7a846-25b1-43c8-bba0-90402485c76e",
      "metadata": {
        "collapsed": true,
        "id": "1ba7a846-25b1-43c8-bba0-90402485c76e"
      },
      "outputs": [],
      "source": [
        "#Dimention Reduction (UMAP)\n",
        "\n",
        "#Finding the optimal number of components\n",
        "tscores = {}\n",
        "for x in range(2,11):\n",
        "    u_map = umap.UMAP(n_components=x)\n",
        "    umap_results = u_map.fit_transform(df)\n",
        "    tscores[x] = trustworthiness(df, umap_results)\n",
        "\n",
        "print(tscores)\n",
        "\n",
        "#Will use 4 components 0.99191370646281 (trust score), after this the value diminishes with the increase in componetns\n",
        "\n",
        "#Apply Umap\n",
        "umap_reduction = umap.UMAP(n_components=4, random_state=23)\n",
        "umap_reduction_results = umap_reduction.fit_transform(other_features_data)\n",
        "\n",
        "#Turn Results to DataFrame\n",
        "umap_df = pd.DataFrame(umap_reduction_results, columns=[\"Umap1\" , \"Umap2\", \"Umap3\", \"Umap4\"])\n",
        "umap_df[\"Season\"] = df[\"Season(Summer (1) or Winter(0))\"].reset_index(drop=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NpHHsYBTpUJx",
      "metadata": {
        "id": "NpHHsYBTpUJx"
      },
      "outputs": [],
      "source": [
        "#Visualize\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(x=\"Umap1\",y=\"Umap2\", hue=\"Season\", data=umap_df, s=100, markers=\"o\")\n",
        "plt.title(label=\"Umap 1 and 2 Plot\")\n",
        "plt.xlabel(\"Umap Component 1\")\n",
        "plt.ylabel(\"Umap Component 2\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(x=\"Umap1\",y=\"Umap3\", hue=\"Season\", data=umap_df, s=100, markers=\"o\")\n",
        "plt.title(label=\"Umap 1 and 3 Plot\")\n",
        "plt.xlabel(\"Umap Component 1\")\n",
        "plt.ylabel(\"Umap Component 3\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(x=\"Umap2\",y=\"Umap3\", hue=\"Season\", data=umap_df, s=100, markers=\"o\")\n",
        "plt.title(label=\"Umap 2 and 3 Plot\")\n",
        "plt.xlabel(\"Umap Component 2\")\n",
        "plt.ylabel(\"Umap Component 3\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VDZRUkY_p7bh",
      "metadata": {
        "id": "VDZRUkY_p7bh"
      },
      "source": [
        "**Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffab2056-3fcb-4f28-b073-945773750de4",
      "metadata": {
        "id": "ffab2056-3fcb-4f28-b073-945773750de4"
      },
      "outputs": [],
      "source": [
        "#Clustering - SpectralClustering on (PCA)\n",
        "\n",
        "#Finding the optimal n_components by finding the highest n_componet and Silhouette Score going through 2 -20 Components\n",
        "silscores = []\n",
        "\n",
        "for x in range(2,21):\n",
        "    spectral_clustering = SpectralClustering(n_components=x, affinity=\"rbf\",random_state =23)\n",
        "    spectral_clustering_results = spectral_clustering.fit_predict(pca_df)\n",
        "    score_spc = silhouette_score(pca_df, spectral_clustering_results)\n",
        "    silscores.append((x, score_spc))\n",
        "\n",
        "best_pair_spc = max(silscores, key=lambda y: y[1])\n",
        "print(best_pair_spc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5194af-de34-4578-87c5-eb6f62eb70c0",
      "metadata": {
        "collapsed": true,
        "id": "2c5194af-de34-4578-87c5-eb6f62eb70c0"
      },
      "outputs": [],
      "source": [
        "#Visualize Clusters - Spectral Clustering\n",
        "real_spectral_cluster = SpectralClustering(n_components=5, affinity=\"rbf\", random_state=23)\n",
        "real_spectral_cluster_results = real_spectral_cluster.fit_predict(pca_df)\n",
        "pca_df[\"real_spectral_cluster_results\"] = real_spectral_cluster_results\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=\"PCA1\", y=\"PCA2\", hue= \"real_spectral_cluster_results\", data=pca_df,  s=100, markers=\"o\")\n",
        "plt.title(\"Spectral Clustering Results on PCA Comp\")\n",
        "plt.xlabel(\"PCA1\")\n",
        "plt.ylabel(\"PCA2\")\n",
        "plt.legend(title=\"Cluster Labels\", loc=\"upper left\", fontsize = \"small\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac4219c-f610-486f-8f5d-1e8841b779ee",
      "metadata": {
        "collapsed": true,
        "id": "dac4219c-f610-486f-8f5d-1e8841b779ee"
      },
      "outputs": [],
      "source": [
        "#Clustering - HDBSCAN on (Umap)\n",
        "\n",
        "#Finding the optimal parameters for min cluster size and min sample size by maximizing the Silhouette score\n",
        "mcs = []\n",
        "ms = []\n",
        "\n",
        "silscores_2 = []\n",
        "\n",
        "for x in range (2,21):\n",
        "    mcs.append(x)\n",
        "    ms.append(x)\n",
        "\n",
        "for i in mcs:\n",
        "    for j in ms:\n",
        "        h_dbscan = hdbscan.HDBSCAN(min_cluster_size=i,min_samples=j)\n",
        "        h_dbscan_results = h_dbscan.fit_predict(umap_df)\n",
        "        if np.any(h_dbscan_results != -1):\n",
        "            score = silhouette_score(umap_df, h_dbscan_results)\n",
        "            silscores_2.append((i,j,score))\n",
        "\n",
        "best_pair_hdbscan = max(silscores_2, key=lambda x: x[2])\n",
        "print(best_pair_hdbscan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aa2839a-caee-48ef-92de-a3ece8cd6360",
      "metadata": {
        "collapsed": true,
        "id": "3aa2839a-caee-48ef-92de-a3ece8cd6360"
      },
      "outputs": [],
      "source": [
        "#Visualize Clusters - HDBSCAN Clustering\n",
        "hdbscan_cluster = hdbscan.HDBSCAN(min_cluster_size=15,min_samples=19)\n",
        "hdbscan_cluster_results = hdbscan_cluster.fit_predict(umap_df)\n",
        "umap_df[\"hdbscan_cluster_results\"] = hdbscan_cluster_results\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x= \"Umap1\", y=\"Umap2\", hue=\"hdbscan_cluster_results\", data=umap_df, s=100, markers=\"o\")\n",
        "plt.title(\"HDBSCAN clustering on Umap Comp\")\n",
        "plt.xlabel(\"Umap1\")\n",
        "plt.ylabel(\"Umap2\")\n",
        "plt.legend(title=\"Cluster Labels\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e81449f-8128-4f43-afeb-78c82f6bc085",
      "metadata": {
        "id": "4e81449f-8128-4f43-afeb-78c82f6bc085"
      },
      "outputs": [],
      "source": [
        "#Clustering - Affinity Propagation on (PCA)\n",
        "\n",
        "#drop the added cluster column form spectral clustering\n",
        "pca_df.drop(\"real_spectral_cluster_results\", axis=1)\n",
        "\n",
        "#Applying Affinity\n",
        "clustering_pca = AffinityPropagation(random_state=23)\n",
        "clustering_pca_results = clustering_pca.fit_predict(pca_df)\n",
        "pca_df[\"Clusters Affinity\"] = clustering_pca_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0pNxmA-THpKt",
      "metadata": {
        "id": "0pNxmA-THpKt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Clustering - Affinity Propagation on (Umap)\n",
        "#drop the added cluster column from HDBSCAN\n",
        "umap_df.drop(\"hdbscan_cluster_results\", axis=1)\n",
        "\n",
        "#Applying Affinity\n",
        "clustering_umap = AffinityPropagation(random_state=23)\n",
        "clustering_umap_results = clustering_umap.fit_predict(umap_df)\n",
        "umap_df[\"Clusters Affinity\"] = clustering_umap_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FmrTUfmTIUmS",
      "metadata": {
        "collapsed": true,
        "id": "FmrTUfmTIUmS"
      },
      "outputs": [],
      "source": [
        "#Visualize\n",
        "\n",
        "#Affinity Propagation on (PCA)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=\"PCA1\", y=\"PCA2\", hue =\"Clusters Affinity\", data=pca_df, s=100, markers=\"o\")\n",
        "plt.title(\"Affinity Propagation Clustering on PCA Comp\")\n",
        "plt.legend(title=\"Cluster Labels\")\n",
        "plt.xlabel(\"PCA1\")\n",
        "plt.ylabel(\"PCA2\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=\"Umap1\", y=\"Umap2\", hue =\"Clusters Affinity\", data=umap_df, s=100, markers=\"o\")\n",
        "plt.title(\"Affinity Propagation Clustering on Umap Comp\")\n",
        "plt.legend(title=\"Cluster Labels\")\n",
        "plt.xlabel(\"Umap1\")\n",
        "plt.ylabel(\"Umap2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fg7tzZy9JcCV",
      "metadata": {
        "id": "Fg7tzZy9JcCV"
      },
      "outputs": [],
      "source": [
        "#Clusters Evaluation - Davies-Bouldin\n",
        "\n",
        "\n",
        "#Spectral Clustering\n",
        "db_spectral_clustering = davies_bouldin_score(pca_results,real_spectral_cluster_results)\n",
        "\n",
        "#HDBSCAN Clustering\n",
        "db_hdbscan_clustering = davies_bouldin_score(umap_reduction_results, hdbscan_cluster_results)\n",
        "\n",
        "#Affinity Propagation Clustering on (PCA)\n",
        "db_ap_pca = davies_bouldin_score(pca_results,clustering_pca_results)\n",
        "\n",
        "#Affinity Propagation Clustering on (Umap)\n",
        "db_ap_umap = davies_bouldin_score(umap_reduction_results,clustering_umap_results)\n",
        "\n",
        "\n",
        "#Print\n",
        "\n",
        "print(f\"The Davies Bouldin Scores: Spectral Clustering: {db_spectral_clustering}, HDBSCAN Clustering: {db_hdbscan_clustering}, Affinity Propagation Clustering on (PCA): {db_ap_pca},Affinity Propagation Clustering on (Umap):{db_ap_umap}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
